{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I4pgzLVtBTP"
      },
      "source": [
        "# 1.0 An end-to-end classification problem (Training)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh34gim6KPtT"
      },
      "source": [
        "## 1.1 Dataset description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE8OJoDZ5AFK"
      },
      "source": [
        "We'll be looking at individual income in the United States. The **data** is from the **1994 census**, and contains information on an individual's **marital status**, **age**, **type of work**, and more. The **target column**, or what we want to predict, is whether individuals make less than or equal to 50k a year, or more than **50k a year**.\n",
        "\n",
        "You can download the data from the [University of California, Irvine's website](http://archive.ics.uci.edu/ml/datasets/Adult).\n",
        "\n",
        "Let's take the following steps:\n",
        "\n",
        "1. ETL (done)\n",
        "2. Data Checks (done)\n",
        "3. Data Segregation (done)\n",
        "4. Training\n",
        "\n",
        "<center><img width=\"800\" src=\"https://drive.google.com/uc?export=view&id=13kfVPzq-Jy-kH6GcJRfB44lIYAu5mtGl\"></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UpxKxU1Ej7f"
      },
      "source": [
        "## 1.2 Install, load libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t82KewAPWCYe"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import wandb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import fbeta_score, precision_score, recall_score, accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "NreRnbvI8lL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to Weights & Biases\n",
        "!wandb login --relogin"
      ],
      "metadata": {
        "id": "QZXcN54GkP25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Holdout Configuration"
      ],
      "metadata": {
        "id": "BeMVUzr08AK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img width=\"800\" src=\"https://drive.google.com/uc?export=view&id=1C2SEVnhyEd_lBpdTuwL3aTxNXh5-ktVX\"></center>"
      ],
      "metadata": {
        "id": "-ExC7k2h7Jb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# global variables\n",
        "\n",
        "# ratio used to split train and validation data\n",
        "val_size = 0.30\n",
        "\n",
        "# seed used to reproduce purposes\n",
        "seed = 41\n",
        "\n",
        "# reference (column) to stratify the data\n",
        "stratify = \"high_income\"\n",
        "\n",
        "# name of the input artifact\n",
        "artifact_input_name = \"decision_tree/train.csv:latest\"\n",
        "\n",
        "# type of the artifact\n",
        "artifact_type = \"Train\""
      ],
      "metadata": {
        "id": "sZzaS0x8ZYUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure logging\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format=\"%(asctime)s %(message)s\",\n",
        "                    datefmt='%d-%m-%Y %H:%M:%S')\n",
        "\n",
        "# reference for a logging obj\n",
        "logger = logging.getLogger()\n",
        "\n",
        "# initiate the wandb project\n",
        "run = wandb.init(project=\"decision_tree\",job_type=\"train\")\n",
        "\n",
        "logger.info(\"Downloading and reading train artifact\")\n",
        "local_path = run.use_artifact(artifact_input_name).file()\n",
        "df_train = pd.read_csv(local_path)\n",
        "\n",
        "# Spliting train.csv into train and validation dataset\n",
        "logger.info(\"Spliting data into train/val\")\n",
        "# split-out train/validation and test dataset\n",
        "x_train, x_val, y_train, y_val = train_test_split(df_train.drop(labels=stratify,axis=1),\n",
        "                                                  df_train[stratify],\n",
        "                                                  test_size=val_size,\n",
        "                                                  random_state=seed,\n",
        "                                                  shuffle=True,\n",
        "                                                  stratify=df_train[stratify])"
      ],
      "metadata": {
        "id": "VILw-BwKY2Wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"x train: {}\".format(x_train.shape))\n",
        "logger.info(\"y train: {}\".format(y_train.shape))\n",
        "logger.info(\"x val: {}\".format(x_val.shape))\n",
        "logger.info(\"y val: {}\".format(y_val.shape))"
      ],
      "metadata": {
        "id": "VOFF72bq9QoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Data preparation"
      ],
      "metadata": {
        "id": "PcqOxTYQ4z3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data preparation** may be the most important part of a machine learning project. \n",
        "\n",
        "```\n",
        "It is the most time-consuming part, although it seems to be the least discussed topic. \n",
        "```\n",
        "\n",
        "**Data preparation** sometimes referred to as **data preprocessing**, is the act of transforming raw data into a form that is appropriate for modeling. \n",
        "\n",
        "Machine learning algorithms require input data to be numbered, and most algorithm implementations maintain this expectation. As such, **if your data contains data types and values that are not numbers, such as labels, you will need to change the data into numbers**. Further, specific machine learning algorithms have expectations regarding the data types, scale, probability distribution, and relationships between input variables, and you may need to change the data to meet these expectations.\n",
        "\n",
        "The philosophy of **data preparation** is to discover how to best expose the unknown underlying structure of the problem to the learning algorithms. This often requires **an iterative path of experimentation through a suite of different data preparation techniques** in order to discover what works well or best. The vast majority of the machine learning algorithms you may use on a project are years to decades old. **The implementation and application of the algorithms are well understood**. So much so that they are routine, with amazing, fully-featured open-source machine learning libraries like [scikit-learn](https://scikit-learn.org/stable/) in Python. \n",
        "```\n",
        "The thing that is different from project to project is the data. \n",
        "```\n",
        "\n",
        "You may be the first person (ever!) to use a specific dataset as the basis for a predictive modeling project. As such, the **preparation of the data** in order to best present it to the problem of the learning algorithms **is the primary task of any modern machine learning project**.\n",
        "\n",
        "````\n",
        "The challenge of data preparation is that each dataset is unique and different.\n",
        "````\n",
        "\n",
        "Datasets differ in the number of variables (tens, hundreds, thousands, or more), the types of the variables (numeric, nominal, ordinal, boolean), the scale of the variables, the drift in the values over time, and more. As such, this makes discussing data preparation a challenge. Either specific case studies are used, or focus is put on the general methods that can be used across projects. The result is that neither approach is explored."
      ],
      "metadata": {
        "id": "XSNQMhXygjSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.1 Outlier Removal"
      ],
      "metadata": {
        "id": "tt9AnGT3922Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img width=\"800\" src=\"https://drive.google.com/uc?export=view&id=1FMN6clRy5WuUPirbNStWFTk3LnxakWWW\"></center>"
      ],
      "metadata": {
        "id": "m3SLeG77DRBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"Outlier Removal\")\n",
        "# temporary variable\n",
        "x = x_train.select_dtypes(\"int64\").copy()\n",
        "\n",
        "# identify outlier in the dataset\n",
        "lof = LocalOutlierFactor()\n",
        "outlier = lof.fit_predict(x)\n",
        "mask = outlier != -1"
      ],
      "metadata": {
        "id": "KRvgo0Ol9sPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"x_train shape [original]: {}\".format(x_train.shape))\n",
        "logger.info(\"x_train shape [outlier removal]: {}\".format(x_train.loc[mask,:].shape))"
      ],
      "metadata": {
        "id": "A2Y4UgfA9-kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AVOID data leakage and you should not do this procedure in the preprocessing stage\n",
        "# Note that we did not perform this procedure in the validation set\n",
        "x_train = x_train.loc[mask,:].copy()\n",
        "y_train = y_train[mask].copy()"
      ],
      "metadata": {
        "id": "TD1yG61E9_xK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.2 Encoding Target Variable"
      ],
      "metadata": {
        "id": "EEKvM3qL_3Hb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img width=\"800\" src=\"https://drive.google.com/uc?export=view&id=1oigyMuJiI5gGb9V185y_GN9skS0BRy5Y\"></center>"
      ],
      "metadata": {
        "id": "9fhdMoJBF2l5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.head(10)"
      ],
      "metadata": {
        "id": "coxDBYZ5Dv4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"Encoding Target Variable\")\n",
        "# define a categorical encoding for target variable\n",
        "le = LabelEncoder()\n",
        "\n",
        "# fit and transform y_train\n",
        "y_train = le.fit_transform(y_train)\n",
        "\n",
        "# transform y_test (avoiding data leakage)\n",
        "y_val = le.transform(y_val)\n",
        "\n",
        "logger.info(\"Classes [0, 1]: {}\".format(le.inverse_transform([0, 1])))"
      ],
      "metadata": {
        "id": "hGToiv4198vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "KjZA7Fa2ENJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val"
      ],
      "metadata": {
        "id": "vHSpkzzdEYpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.3 Encoding independent variables [Experiment]\n"
      ],
      "metadata": {
        "id": "7c_W3F0IGN15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.head()"
      ],
      "metadata": {
        "id": "DB7S-d4nGkzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.workclass.unique()"
      ],
      "metadata": {
        "id": "sAFxm2RV6VLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[\"sex\"].values.shape"
      ],
      "metadata": {
        "id": "Nz1tALP27B8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[\"sex\"].values.reshape(-1,1).shape"
      ],
      "metadata": {
        "id": "Wo0tYaDo7GN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# just an experimentation\n",
        "\n",
        "# drop=first erase redundant column\n",
        "onehot = OneHotEncoder(sparse=False,drop=\"first\")\n",
        "\n",
        "# fit using x_train\n",
        "onehot.fit(x_train[\"sex\"].values.reshape(-1,1))\n",
        "\n",
        "# make a copy\n",
        "x_train_aux = x_train.copy()\n",
        "\n",
        "# transform train \n",
        "x_train_aux[onehot.get_feature_names_out()] = onehot.transform(x_train_aux[\"sex\"].values.reshape(-1,1))\n",
        "x_train_aux.head()"
      ],
      "metadata": {
        "id": "e9HSLsUuGY5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inverse transformation\n",
        "onehot.inverse_transform(np.array([0,1]).reshape(-1,1))"
      ],
      "metadata": {
        "id": "JdD7W-KWIhfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inverse transformation (other way)\n",
        "onehot.inverse_transform([[0],[1]])"
      ],
      "metadata": {
        "id": "46EsKOykJral"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Return the name of the new feature\n",
        "onehot.get_feature_names_out()"
      ],
      "metadata": {
        "id": "Plr9Mw7UJ44T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.4 Encoding independent variables"
      ],
      "metadata": {
        "id": "TuoN4_opKXzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the shape of x train before transformation\n",
        "x_train.shape"
      ],
      "metadata": {
        "id": "uEkoYcQNLmyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets review what are categorical columns\n",
        "x_train.select_dtypes(\"object\").columns.to_list()"
      ],
      "metadata": {
        "id": "YFMnbI6tKbX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 08 columns are \"object\", lets transform them to categorical \n",
        "for name in x_train.select_dtypes(\"object\").columns.to_list():\n",
        "    onehot = OneHotEncoder(sparse=False,drop=\"first\")\n",
        "    # fit using x_train\n",
        "    onehot.fit(x_train[name].values.reshape(-1,1))\n",
        "\n",
        "    # transform train and validation\n",
        "    x_train[onehot.get_feature_names_out()] = onehot.transform(x_train[name].values.reshape(-1,1))\n",
        "    x_val[onehot.get_feature_names_out()] = onehot.transform(x_val[name].values.reshape(-1,1))"
      ],
      "metadata": {
        "id": "mm8VoPdtKmmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.head()"
      ],
      "metadata": {
        "id": "UspBaLsuLUzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_val.head()"
      ],
      "metadata": {
        "id": "aZEiTzwRLg4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols=['workclass','education','marital_status','occupation',\n",
        "      'relationship','race','sex','native_country']\n",
        "\n",
        "x_train.drop(labels=cols,axis=1,inplace=True)\n",
        "x_val.drop(labels=cols,axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "TrkGdT4QLzBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.head()"
      ],
      "metadata": {
        "id": "MqhsXXzPL5ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_val.head()"
      ],
      "metadata": {
        "id": "HSRjrsQuL75B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In thesis, after this encoding process the dataset is ready to be trained. "
      ],
      "metadata": {
        "id": "zXOAm38WZXIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# close the current run before to execute the next section\n",
        "run.finish()"
      ],
      "metadata": {
        "id": "V9cMG590cum8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.5 Using a full-pipeline"
      ],
      "metadata": {
        "id": "7qcpCDX0gce4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "A reproducible pipeline is all you need\n",
        "```\n",
        "\n",
        "To follow the next cells' execution, it is important you re-run all cells at:\n",
        "- Section 1.3\n",
        "- Section 1.4.1\n",
        "- Section 1.4.2\n",
        "\n",
        "These steps are necessary to guarantee the raw ```train.csv``` is used."
      ],
      "metadata": {
        "id": "rNRBHjI-ZscT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4.5.1 Feature Extractor"
      ],
      "metadata": {
        "id": "gF_ZLoVjjPUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
        "    # Class Constructor\n",
        "    def __init__(self, feature_names):\n",
        "        self.feature_names = feature_names\n",
        "\n",
        "    # Return self nothing else to do here\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    # Method that describes what this custom transformer need to do\n",
        "    def transform(self, X, y=None):\n",
        "        return X[self.feature_names]"
      ],
      "metadata": {
        "id": "0yn1GUbCdFmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for validation purposes\n",
        "fs = FeatureSelector(x_train.select_dtypes(\"object\").columns.to_list())\n",
        "df = fs.fit_transform(x_train)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "DEa9qLnVirdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for validation purposes\n",
        "fs = FeatureSelector(x_train.select_dtypes(\"int64\").columns.to_list())\n",
        "df = fs.fit_transform(x_train)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "9GQeBeNNjB8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4.5.2 Handling Categorical Features"
      ],
      "metadata": {
        "id": "_ofrw3WLjVay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling categorical features\n",
        "class CategoricalTransformer(BaseEstimator, TransformerMixin):\n",
        "    # Class constructor method that takes one boolean as its argument\n",
        "    def __init__(self, new_features=True, colnames=None):\n",
        "        self.new_features = new_features\n",
        "        self.colnames = colnames\n",
        "\n",
        "    # Return self nothing else to do here\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def get_feature_names_out(self):\n",
        "        return self.colnames.tolist()\n",
        "\n",
        "    # Transformer method we wrote for this transformer\n",
        "    def transform(self, X, y=None):\n",
        "        df = pd.DataFrame(X, columns=self.colnames)\n",
        "\n",
        "        # Remove white space in categorical features\n",
        "        df = df.apply(lambda row: row.str.strip())\n",
        "\n",
        "        # customize feature?\n",
        "        # How can I identify what needs to be modified? EDA!!!!\n",
        "        if self.new_features:\n",
        "\n",
        "            # minimize the cardinality of native_country feature\n",
        "            # check cardinality using df.native_country.unique()\n",
        "            df.loc[df['native_country'] != 'United-States','native_country'] = 'non_usa'\n",
        "\n",
        "            # replace ? with Unknown\n",
        "            edit_cols = ['native_country', 'occupation', 'workclass']\n",
        "            for col in edit_cols:\n",
        "                df.loc[df[col] == '?', col] = 'unknown'\n",
        "\n",
        "            # decrease the cardinality of education feature\n",
        "            hs_grad = ['HS-grad', '11th', '10th', '9th', '12th']\n",
        "            elementary = ['1st-4th', '5th-6th', '7th-8th']\n",
        "            # replace\n",
        "            df['education'].replace(to_replace=hs_grad,value='HS-grad',inplace=True)\n",
        "            df['education'].replace(to_replace=elementary,value='elementary_school',inplace=True)\n",
        "\n",
        "            # adjust marital_status feature\n",
        "            married = ['Married-spouse-absent','Married-civ-spouse','Married-AF-spouse']\n",
        "            separated = ['Separated', 'Divorced']\n",
        "\n",
        "            # replace\n",
        "            df['marital_status'].replace(to_replace=married, value='Married', inplace=True)\n",
        "            df['marital_status'].replace(to_replace=separated, value='Separated', inplace=True)\n",
        "\n",
        "            # adjust workclass feature\n",
        "            self_employed = ['Self-emp-not-inc', 'Self-emp-inc']\n",
        "            govt_employees = ['Local-gov', 'State-gov', 'Federal-gov']\n",
        "\n",
        "            # replace elements in list.\n",
        "            df['workclass'].replace(to_replace=self_employed,value='Self_employed',inplace=True)\n",
        "            df['workclass'].replace(to_replace=govt_employees,value='Govt_employees',inplace=True)\n",
        "\n",
        "        # update column names\n",
        "        self.colnames = df.columns\n",
        "\n",
        "        return df\n"
      ],
      "metadata": {
        "id": "Od5QTxwdhqgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for validation purposes\n",
        "fs = FeatureSelector(x_train.select_dtypes(\"object\").columns.to_list())\n",
        "df = fs.fit_transform(x_train)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "j6BYQho9mQHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for validation purposes\n",
        "ct = CategoricalTransformer(new_features=True,colnames=df.columns.tolist())\n",
        "df_cat = ct.fit_transform(df)\n",
        "df_cat.head()"
      ],
      "metadata": {
        "id": "41xrQMJqmfEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the cardinality before and after transformation\n",
        "x_train.select_dtypes(\"object\").apply(pd.Series.nunique)"
      ],
      "metadata": {
        "id": "jmznhBWqo9zE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the cardinality before and after transformation\n",
        "df_cat.apply(pd.Series.nunique)"
      ],
      "metadata": {
        "id": "ig-jZN0NpCTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4.5.3 Handling Numerical Features"
      ],
      "metadata": {
        "id": "ldfU54n7njja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transform numerical features\n",
        "class NumericalTransformer(BaseEstimator, TransformerMixin):\n",
        "    # Class constructor method that takes a model parameter as its argument\n",
        "    # model 0: minmax\n",
        "    # model 1: standard\n",
        "    # model 2: without scaler\n",
        "    def __init__(self, model=0, colnames=None):\n",
        "        self.model = model\n",
        "        self.colnames = colnames\n",
        "        self.scaler = None\n",
        "\n",
        "    # Fit is used only to learn statistical about Scalers\n",
        "    def fit(self, X, y=None):\n",
        "        df = pd.DataFrame(X, columns=self.colnames)\n",
        "        # minmax\n",
        "        if self.model == 0:\n",
        "            self.scaler = MinMaxScaler()\n",
        "            self.scaler.fit(df)\n",
        "        # standard scaler\n",
        "        elif self.model == 1:\n",
        "            self.scaler = StandardScaler()\n",
        "            self.scaler.fit(df)\n",
        "        return self\n",
        "\n",
        "    # return columns names after transformation\n",
        "    def get_feature_names_out(self):\n",
        "        return self.colnames\n",
        "\n",
        "    # Transformer method we wrote for this transformer\n",
        "    # Use fitted scalers\n",
        "    def transform(self, X, y=None):\n",
        "        df = pd.DataFrame(X, columns=self.colnames)\n",
        "\n",
        "        # update columns name\n",
        "        self.colnames = df.columns.tolist()\n",
        "\n",
        "        # minmax\n",
        "        if self.model == 0:\n",
        "            # transform data\n",
        "            df = self.scaler.transform(df)\n",
        "        elif self.model == 1:\n",
        "            # transform data\n",
        "            df = self.scaler.transform(df)\n",
        "        else:\n",
        "            df = df.values\n",
        "\n",
        "        return df"
      ],
      "metadata": {
        "id": "2RWfY7ctjxW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for validation purposes\n",
        "fs = FeatureSelector(x_train.select_dtypes(\"int64\").columns.to_list())\n",
        "df = fs.fit_transform(x_train)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "05vfbEtjo4RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for validation purposes\n",
        "nt = NumericalTransformer(model=1)\n",
        "df_num = nt.fit_transform(df)\n",
        "df_num"
      ],
      "metadata": {
        "id": "22lgbdTqpXqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# return columns name\n",
        "nt.get_feature_names_out()"
      ],
      "metadata": {
        "id": "GgJK5Bnspo8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4.5.4 Data Preparation Pipeline"
      ],
      "metadata": {
        "id": "_HjGtsdP2xb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = 0 (min-max), 1 (z-score), 2 (without normalization)\n",
        "numerical_model = 0\n",
        "\n",
        "# Categrical features to pass down the categorical pipeline\n",
        "categorical_features = x_train.select_dtypes(\"object\").columns.to_list()\n",
        "\n",
        "# Numerical features to pass down the numerical pipeline\n",
        "numerical_features = x_train.select_dtypes(\"int64\").columns.to_list()\n",
        "\n",
        "# Defining the steps for the categorical pipeline\n",
        "categorical_pipeline = Pipeline(steps=[('cat_selector', FeatureSelector(categorical_features)),\n",
        "                                       ('imputer_cat', SimpleImputer(strategy=\"most_frequent\")),\n",
        "                                       ('cat_transformer', CategoricalTransformer(colnames=categorical_features)),\n",
        "                                       # ('cat_encoder','passthrough'\n",
        "                                       ('cat_encoder', OneHotEncoder(sparse=False, drop=\"first\"))\n",
        "                                       ]\n",
        "                                )\n",
        "\n",
        "# Defining the steps in the numerical pipeline\n",
        "numerical_pipeline = Pipeline(steps=[('num_selector', FeatureSelector(numerical_features)),\n",
        "                                     ('imputer_num', SimpleImputer(strategy=\"median\")),\n",
        "                                     ('num_transformer', NumericalTransformer(numerical_model, \n",
        "                                                                              colnames=numerical_features))])\n",
        "\n",
        "# Combine numerical and categorical pieplines into one full big pipeline horizontally\n",
        "full_pipeline_preprocessing = FeatureUnion(transformer_list=[('cat_pipeline', categorical_pipeline),\n",
        "                                                             ('num_pipeline', numerical_pipeline)]\n",
        "                                           )"
      ],
      "metadata": {
        "id": "dOn6aApB212h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for validation purposes\n",
        "new_data = full_pipeline_preprocessing.fit_transform(x_train)\n",
        "# cat_names is a numpy array\n",
        "cat_names = full_pipeline_preprocessing.get_params()[\"cat_pipeline\"][3].get_feature_names_out().tolist()\n",
        "# num_names is a list\n",
        "num_names = full_pipeline_preprocessing.get_params()[\"num_pipeline\"][2].get_feature_names_out()\n",
        "df = pd.DataFrame(new_data,columns = cat_names + num_names)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "eKOidk7l4PXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Holdout Training"
      ],
      "metadata": {
        "id": "3JBkxykTp2HF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example the inference artifacts are:\n",
        "- pipe (contains trained model and the preprocesing pipeline\n",
        "- le (label encoder object used to encode the target variable)"
      ],
      "metadata": {
        "id": "_toW-9SWAZv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The full pipeline \n",
        "pipe = Pipeline(steps = [('full_pipeline', full_pipeline_preprocessing),\n",
        "                         (\"classifier\",DecisionTreeClassifier())\n",
        "                         ]\n",
        "                )\n",
        "\n",
        "# training\n",
        "logger.info(\"Training\")\n",
        "pipe.fit(x_train, y_train)\n",
        "\n",
        "# predict\n",
        "logger.info(\"Infering\")\n",
        "predict = pipe.predict(x_val)\n",
        "\n",
        "# Evaluation Metrics\n",
        "logger.info(\"Evaluation metrics\")\n",
        "fbeta = fbeta_score(y_val, predict, beta=1, zero_division=1)\n",
        "precision = precision_score(y_val, predict, zero_division=1)\n",
        "recall = recall_score(y_val, predict, zero_division=1)\n",
        "acc = accuracy_score(y_val, predict)\n",
        "\n",
        "logger.info(\"Accuracy: {}\".format(acc))\n",
        "logger.info(\"Precision: {}\".format(precision))\n",
        "logger.info(\"Recall: {}\".format(recall))\n",
        "logger.info(\"F1: {}\".format(fbeta))"
      ],
      "metadata": {
        "id": "v-CIUlIE2aVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run.summary[\"Acc\"] = acc\n",
        "run.summary[\"Precision\"] = precision\n",
        "run.summary[\"Recall\"] = recall\n",
        "run.summary[\"F1\"] = fbeta"
      ],
      "metadata": {
        "id": "kw1kSZf37-yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict bias\n",
        "logger.info(\"Infering Bias\")\n",
        "predict_bias = pipe.predict(x_train)\n",
        "\n",
        "# Evaluation Metrics\n",
        "logger.info(\"Bias Evaluation metrics\")\n",
        "fbeta = fbeta_score(y_train, predict_bias, beta=1, zero_division=1)\n",
        "precision = precision_score(y_train, predict_bias, zero_division=1)\n",
        "recall = recall_score(y_train, predict_bias, zero_division=1)\n",
        "acc = accuracy_score(y_train, predict_bias)\n",
        "\n",
        "logger.info(\"Bias Accuracy: {}\".format(acc))\n",
        "logger.info(\"Bias Precision: {}\".format(precision))\n",
        "logger.info(\"Bias Recall: {}\".format(recall))\n",
        "logger.info(\"Bias F1: {}\".format(fbeta))"
      ],
      "metadata": {
        "id": "wcIO140lO-xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the accuracy, precision, recall with previous ones\n",
        "print(classification_report(y_val,predict))"
      ],
      "metadata": {
        "id": "RR7x-wAM94VS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig_confusion_matrix, ax = plt.subplots(1,1,figsize=(7,4))\n",
        "ConfusionMatrixDisplay(confusion_matrix(predict,y_val,labels=[1,0]),\n",
        "                       display_labels=[\">50k\",\"<=50k\"]).plot(values_format=\".0f\",ax=ax)\n",
        "\n",
        "ax.set_xlabel(\"True Label\")\n",
        "ax.set_ylabel(\"Predicted Label\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9f5FubtsA8yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploading figures\n",
        "logger.info(\"Uploading figures\")\n",
        "run.log(\n",
        "    {\n",
        "        \"confusion_matrix\": wandb.Image(fig_confusion_matrix),\n",
        "        # \"other_figure\": wandb.Image(other_fig)\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "7NODonon9kHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature importance\n",
        "pipe.get_params()[\"classifier\"].feature_importances_"
      ],
      "metadata": {
        "id": "k1XylahaKaCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get categorical column names\n",
        "cat_names = pipe.named_steps['full_pipeline'].get_params()[\"cat_pipeline\"][3].get_feature_names_out().tolist()\n",
        "cat_names"
      ],
      "metadata": {
        "id": "TCp9zEgGMJD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get numerical column names\n",
        "num_names = pipe.named_steps['full_pipeline'].get_params()[\"num_pipeline\"][2].get_feature_names_out()\n",
        "num_names"
      ],
      "metadata": {
        "id": "6Ob7S9N2L82x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge all column names together\n",
        "all_names = cat_names + num_names\n",
        "all_names"
      ],
      "metadata": {
        "id": "USlqpKBUMV64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize all classifier plots\n",
        "# For a complete documentation please see: https://docs.wandb.ai/guides/integrations/scikit\n",
        "wandb.sklearn.plot_classifier(pipe.get_params()[\"classifier\"],\n",
        "                              full_pipeline_preprocessing.transform(x_train),\n",
        "                              full_pipeline_preprocessing.transform(x_val),\n",
        "                              y_train,\n",
        "                              y_val,\n",
        "                              predict,\n",
        "                              pipe.predict_proba(x_val),\n",
        "                              [0,1],\n",
        "                              model_name='DT', feature_names=all_names)"
      ],
      "metadata": {
        "id": "wo0jKHj0FlIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize single plot\n",
        "# wandb.sklearn.plot_confusion_matrix(y_val, predict, [0,1])"
      ],
      "metadata": {
        "id": "ZZa7WZFCD9KE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Catch up the class proportions between train and validation\n",
        "# wandb.sklearn.plot_class_proportions(y_train, y_val, [0,1])"
      ],
      "metadata": {
        "id": "cd9LLU0sFw5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the importance of columns\n",
        "# wandb.sklearn.plot_feature_importances(pipe.get_params()[\"classifier\"],all_names)"
      ],
      "metadata": {
        "id": "Wd16dY-3Mn2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot a brief summary of the weighted avg results (recall, precision, f1, acc)\n",
        "# wandb.sklearn.plot_summary_metrics(pipe.get_params()[\"classifier\"],\n",
        "#                                    full_pipeline_preprocessing.transform(x_train),\n",
        "#                                    y_train,\n",
        "#                                    full_pipeline_preprocessing.transform(x_val),\n",
        "#                                    y_val)"
      ],
      "metadata": {
        "id": "CGX5BdLpMwKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROC curve\n",
        "# predict_proba = pipe.predict_proba(x_val)\n",
        "# wandb.sklearn.plot_roc(y_val, predict_proba, [0,1])"
      ],
      "metadata": {
        "id": "D7Sl-7TbOZqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# close the current run before to execute the next section\n",
        "run.finish()"
      ],
      "metadata": {
        "id": "v7WQb9yaReCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.6 Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "G8tKLeBIa44E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To see all parameters of a specific step of the pipeline\n",
        "# pipe.named_steps['classifier'].get_params() #or\n",
        "# pipe.named_steps['full_pipeline'].get_params()"
      ],
      "metadata": {
        "id": "eBVoapcIkc6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# global seed\n",
        "seed = 41"
      ],
      "metadata": {
        "id": "jPy291-mwumB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    # try grid or random\n",
        "    \"method\": \"random\",\n",
        "    \"metric\": {\n",
        "        \"name\": \"Accuracy\",\n",
        "        \"goal\": \"maximize\"\n",
        "        },\n",
        "    \"parameters\": {\n",
        "        \"criterion\": {\n",
        "            \"values\": [\"gini\",\"entropy\"]\n",
        "            },\n",
        "        \"splitter\": {\n",
        "            \"values\": [\"random\",\"best\"]\n",
        "        },\n",
        "        \"model\": {\n",
        "            \"values\": [0,1,2]\n",
        "        },\n",
        "        \"random_state\": {\n",
        "            \"values\": [seed]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"decision_tree\")"
      ],
      "metadata": {
        "id": "fMuG1XSTnXVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    with wandb.init() as run:\n",
        "\n",
        "        # The full pipeline \n",
        "        pipe = Pipeline(steps = [('full_pipeline', full_pipeline_preprocessing),\n",
        "                                    (\"classifier\",DecisionTreeClassifier())\n",
        "                                    ]\n",
        "                        )\n",
        "\n",
        "        # update the parameters of the pipeline that we would like to tuning\n",
        "        pipe.set_params(**{\"full_pipeline__num_pipeline__num_transformer__model\": run.config.model})\n",
        "        pipe.set_params(**{\"classifier__criterion\": run.config.criterion})\n",
        "        pipe.set_params(**{\"classifier__splitter\": run.config.splitter})\n",
        "        pipe.set_params(**{\"classifier__random_state\": run.config.random_state})\n",
        "\n",
        "        # training\n",
        "        logger.info(\"Training\")\n",
        "        pipe.fit(x_train, y_train)\n",
        "\n",
        "        # predict\n",
        "        logger.info(\"Infering\")\n",
        "        predict = pipe.predict(x_val)\n",
        "\n",
        "        # Evaluation Metrics\n",
        "        logger.info(\"Evaluation metrics\")\n",
        "        fbeta = fbeta_score(y_val, predict, beta=1, zero_division=1)\n",
        "        precision = precision_score(y_val, predict, zero_division=1)\n",
        "        recall = recall_score(y_val, predict, zero_division=1)\n",
        "        acc = accuracy_score(y_val, predict)\n",
        "\n",
        "        logger.info(\"Accuracy: {}\".format(acc))\n",
        "        logger.info(\"Precision: {}\".format(precision))\n",
        "        logger.info(\"Recall: {}\".format(recall))\n",
        "        logger.info(\"F1: {}\".format(fbeta))\n",
        "\n",
        "        run.summary[\"Accuracy\"] = acc\n",
        "        run.summary[\"Precision\"] = precision\n",
        "        run.summary[\"Recall\"] = recall\n",
        "        run.summary[\"F1\"] = fbeta"
      ],
      "metadata": {
        "id": "uvnUejepa_FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(sweep_id, train, count=8)"
      ],
      "metadata": {
        "id": "W1hb9Y4ncZel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.7 Configure and train the best model"
      ],
      "metadata": {
        "id": "HKRubLTMopvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that a new run is yet synced to last sweep run\n",
        "# Just to check\n",
        "# run = wandb.init()"
      ],
      "metadata": {
        "id": "IGYp599tSSfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Important</font> to restart the colab to unlink a new experiment (run) with the last ```sweep``` experiment. \n",
        "\n",
        "```\n",
        "Runtime >> Factory reset runtime\n",
        "```\n",
        "> Re-run all cells except for: ```1.4.3```, ```1.4.4```, ```1.5``` and ```1.6```. "
      ],
      "metadata": {
        "id": "WlDpHMw8SBJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initiate the wandb project\n",
        "# run = wandb.init(project=\"decision_tree\",job_type=\"train\")"
      ],
      "metadata": {
        "id": "UV3uATIHHxsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The full pipeline \n",
        "pipe = Pipeline(steps = [('full_pipeline', full_pipeline_preprocessing),\n",
        "                         (\"classifier\",DecisionTreeClassifier())\n",
        "                         ]\n",
        "                )\n",
        "\n",
        "# update the parameters of the pipeline that we would like to tuning\n",
        "pipe.set_params(**{\"full_pipeline__num_pipeline__num_transformer__model\": 2})\n",
        "pipe.set_params(**{\"classifier__criterion\": 'entropy'})\n",
        "pipe.set_params(**{\"classifier__splitter\": 'best'})\n",
        "pipe.set_params(**{\"classifier__random_state\": 41})\n",
        "\n",
        "\n",
        "\n",
        "# training\n",
        "logger.info(\"Training\")\n",
        "pipe.fit(x_train, y_train)\n",
        "\n",
        "# predict\n",
        "logger.info(\"Infering\")\n",
        "predict = pipe.predict(x_val)\n",
        "\n",
        "# Evaluation Metrics\n",
        "logger.info(\"Evaluation metrics\")\n",
        "fbeta = fbeta_score(y_val, predict, beta=1, zero_division=1)\n",
        "precision = precision_score(y_val, predict, zero_division=1)\n",
        "recall = recall_score(y_val, predict, zero_division=1)\n",
        "acc = accuracy_score(y_val, predict)\n",
        "\n",
        "logger.info(\"Accuracy: {}\".format(acc))\n",
        "logger.info(\"Precision: {}\".format(precision))\n",
        "logger.info(\"Recall: {}\".format(recall))\n",
        "logger.info(\"F1: {}\".format(fbeta))\n",
        "\n",
        "run.summary[\"Acc\"] = acc\n",
        "run.summary[\"Precision\"] = precision\n",
        "run.summary[\"Recall\"] = recall\n",
        "run.summary[\"F1\"] = fbeta"
      ],
      "metadata": {
        "id": "suQFb9M5HM1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get categorical column names\n",
        "cat_names = pipe.named_steps['full_pipeline'].get_params()[\"cat_pipeline\"][3].get_feature_names_out().tolist()\n",
        "cat_names"
      ],
      "metadata": {
        "id": "CD_vARrbIXv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get numerical column names\n",
        "num_names = pipe.named_steps['full_pipeline'].get_params()[\"num_pipeline\"][2].get_feature_names_out()\n",
        "num_names"
      ],
      "metadata": {
        "id": "g1SkdIXBIXv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge all column names together\n",
        "all_names = cat_names + num_names\n",
        "all_names"
      ],
      "metadata": {
        "id": "PYhk1nW4IXv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize all classifier plots\n",
        "# For a complete documentation please see: https://docs.wandb.ai/guides/integrations/scikit\n",
        "wandb.sklearn.plot_classifier(pipe.get_params()[\"classifier\"],\n",
        "                              full_pipeline_preprocessing.transform(x_train),\n",
        "                              full_pipeline_preprocessing.transform(x_val),\n",
        "                              y_train,\n",
        "                              y_val,\n",
        "                              predict,\n",
        "                              pipe.predict_proba(x_val),\n",
        "                              [0,1],\n",
        "                              model_name='BestModel', feature_names=all_names)"
      ],
      "metadata": {
        "id": "nJwRFzuaIXv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.8 Export the best model"
      ],
      "metadata": {
        "id": "zdJlBFpQy60F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# types and names of the artifacts\n",
        "artifact_type = \"inference_artifact\"\n",
        "artifact_encoder = \"target_encoder\"\n",
        "artifact_model = \"model_export\""
      ],
      "metadata": {
        "id": "-5KDt6Pu0Xo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"Dumping the artifacts to disk\")\n",
        "# Save the model using joblib\n",
        "joblib.dump(pipe, artifact_model)\n",
        "\n",
        "# Save the target encoder using joblib\n",
        "joblib.dump(le, artifact_encoder)"
      ],
      "metadata": {
        "id": "BEIC9zC55mP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model artifact\n",
        "artifact = wandb.Artifact(artifact_model,\n",
        "                          type=artifact_type,\n",
        "                          description=\"A full pipeline composed of a Preprocessing Stage and a Decision Tree model\"\n",
        "                          )\n",
        "\n",
        "logger.info(\"Logging model artifact\")\n",
        "artifact.add_file(artifact_model)\n",
        "run.log_artifact(artifact)"
      ],
      "metadata": {
        "id": "5ncI2WnK5iZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Target encoder artifact\n",
        "artifact = wandb.Artifact(artifact_encoder,\n",
        "                          type=artifact_type,\n",
        "                          description=\"The encoder used to encode the target variable\"\n",
        "                          )\n",
        "\n",
        "logger.info(\"Logging target enconder artifact\")\n",
        "artifact.add_file(artifact_encoder)\n",
        "run.log_artifact(artifact)"
      ],
      "metadata": {
        "id": "ydqNDU3BVm5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run.finish()"
      ],
      "metadata": {
        "id": "6OqCWx5q7fqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "auzejq650EvX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}