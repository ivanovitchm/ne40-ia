{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_segregation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I4pgzLVtBTP"
      },
      "source": [
        "# 1.0 An end-to-end classification problem (Data Segregation)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh34gim6KPtT"
      },
      "source": [
        "## 1.1 Dataset description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE8OJoDZ5AFK"
      },
      "source": [
        "We'll be looking at individual income in the United States. The **data** is from the **1994 census**, and contains information on an individual's **marital status**, **age**, **type of work**, and more. The **target column**, or what we want to predict, is whether individuals make less than or equal to 50k a year, or more than **50k a year**.\n",
        "\n",
        "You can download the data from the [University of California, Irvine's website](http://archive.ics.uci.edu/ml/datasets/Adult).\n",
        "\n",
        "Let's take the following steps:\n",
        "\n",
        "1. ETL (done)\n",
        "2. Data Checks (done)\n",
        "3. Data Segregation\n",
        "\n",
        "<center><img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1a-nyAPNPiVh-Xb2Pu2t2p-BhSvHJS0pO\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UpxKxU1Ej7f"
      },
      "source": [
        "## 1.2 Install, load libraries and setup wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t82KewAPWCYe"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LASaVZuhRJlL"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import tempfile\n",
        "import pandas as pd\n",
        "import os\n",
        "import wandb\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to Weights & Biases\n",
        "!wandb login --relogin"
      ],
      "metadata": {
        "id": "QZXcN54GkP25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Data Segregation"
      ],
      "metadata": {
        "id": "sa1PvzZYF4J2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# global variables\n",
        "\n",
        "# ratio used to split train and test data\n",
        "test_size = 0.30\n",
        "\n",
        "# seed used to reproduce purposes\n",
        "seed = 41\n",
        "\n",
        "# reference (column) to stratify the data\n",
        "stratify = \"high_income\"\n",
        "\n",
        "# name of the input artifact\n",
        "artifact_input_name = \"decision_tree/preprocessed_data.csv:latest\"\n",
        "\n",
        "# type of the artifact\n",
        "artifact_type = \"segregated_data\""
      ],
      "metadata": {
        "id": "6Box9NiNKCgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure logging\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format=\"%(asctime)s %(message)s\",\n",
        "                    datefmt='%d-%m-%Y %H:%M:%S')\n",
        "\n",
        "# reference for a logging obj\n",
        "logger = logging.getLogger()\n",
        "\n",
        "# initiate wandb project\n",
        "run = wandb.init(project=\"decision_tree\", job_type=\"split_data\")\n",
        "\n",
        "logger.info(\"Downloading and reading artifact\")\n",
        "artifact = run.use_artifact(artifact_input_name)\n",
        "artifact_path = artifact.file()\n",
        "df = pd.read_csv(artifact_path)\n",
        "\n",
        "# Split firstly in train/test, then we further divide the dataset to train and validation\n",
        "logger.info(\"Splitting data into train and test\")\n",
        "splits = {}\n",
        "\n",
        "splits[\"train\"], splits[\"test\"] = train_test_split(df,\n",
        "                                                   test_size=test_size,\n",
        "                                                   random_state=seed,\n",
        "                                                   stratify=df[stratify])\n",
        "\n",
        "# Save the artifacts. We use a temporary directory so we do not leave any trace behind\n",
        "with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "\n",
        "    for split, df in splits.items():\n",
        "\n",
        "        # Make the artifact name from the name of the split plus the provided root\n",
        "        artifact_name = f\"{split}.csv\"\n",
        "\n",
        "        # Get the path on disk within the temp directory\n",
        "        temp_path = os.path.join(tmp_dir, artifact_name)\n",
        "\n",
        "        logger.info(f\"Uploading the {split} dataset to {artifact_name}\")\n",
        "\n",
        "        # Save then upload to W&B\n",
        "        df.to_csv(temp_path,index=False)\n",
        "\n",
        "        artifact = wandb.Artifact(name=artifact_name,\n",
        "                                  type=artifact_type,\n",
        "                                  description=f\"{split} split of dataset {artifact_input_name}\",\n",
        "        )\n",
        "        artifact.add_file(temp_path)\n",
        "\n",
        "        logger.info(\"Logging artifact\")\n",
        "        run.log_artifact(artifact)\n",
        "\n",
        "        # This waits for the artifact to be uploaded to W&B. If you\n",
        "        # do not add this, the temp directory might be removed before\n",
        "        # W&B had a chance to upload the datasets, and the upload\n",
        "        # might fail\n",
        "        artifact.wait()"
      ],
      "metadata": {
        "id": "4tha7oPLF58G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# close the run\n",
        "# waiting a while after run the previous cell before execute this\n",
        "run.finish()"
      ],
      "metadata": {
        "id": "s1IcDCzUO57y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "s-decyPSPZ_d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}